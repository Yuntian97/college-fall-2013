\documentclass{article}

\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[usenames,dvipsnames]{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{changepage}
\usepackage{lineno}
\usepackage{algpseudocode}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

\newenvironment{homeworkProblem}{
    \section{Problem \arabic{homeworkProblemCounter}}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\newcommand{\hmwkTitle}{Homework\ \#4}
\newcommand{\hmwkDueDate}{October 11, 2013 at 4:30pm}
\newcommand{\hmwkClass}{CS311}
\newcommand{\hmwkClassTime}{Section 3}
\newcommand{\hmwkClassInstructor}{Professor Lathrop}
\newcommand{\hmwkAuthorName}{Josh Davis}

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}
    Consider the binary search tree code given and prove the two propositions.
    \\

    \textbf{Solution}
    \\

    \textbf{Part One}
    Prove that the best case running time of \alg{BUILD-BST} is \(\Omega(n \lg n)\).

    \begin{proof}
        To prove that the best case running time of \alg{BUILD-BST} is
        \(\Omega(n \lg n)\), we will define a recurrence for \alg{BUILD-BST},
        \(T(n)\), and then show that \(T(n) \in \Omega(n \lg n)\) by showing
        that \(0 \leq c \cdot n \lg n \leq T(n)\) for some constant \(c > 0\)
        and all \(n \geq n_0\).
        \\

        To prove the worst case, we need to understand how \alg{BST.add()}
        works.  It recurses into either the left or right subtree dependent on
        the values of each node.
        \\

        To illicit best case run-time, it follows that during each recursion,
        we cut down the size of our problem by a factor of 2. This would give
        the recurrence as follows: \(T(n) = T(n/2) + \Theta(1)\) where the work
        done at each step is constant. We can now use the master method.
        \\

        By applying the master method, we get \(a = 1\), \(b = 2\) and \(f(n) =
        \Theta(1)\).  Which gives \(n^{\log_b a} = n^{\log_2 1} = n^0 = 1\).
        Since \(f(n) \in \Theta(1)\), this puts us into the second case where
        \(T(n) = \Theta(n \lg n)\).
        \\

        By using the following knowledge:
        \[
            \exists c_1 \exists c_2 \forall n \geq n_0,\ {c_1 \cdot g(n) \leq
            f(n) \leq c_2 \cdot g(n)}
        \]

        It follows that when a function \(T(n) \in \Theta(g(n))\), it also is
        \(T(n) \in \Omega(g(n))\) by the definition of \(\Theta(n)\). Thus we have
        proven the best case running time of \alg{BUILD-BST} is \(\Omega(n \lg
        n)\).
    \end{proof}

    \textbf{Part Two}
    Prove that the worst case running time of \alg{BUILD-BST} is \(O(n^2)\).

    \begin{proof}
        To prove that the worst case running time of \alg{BUILD-BST} is
        \(O(n^2)\), we will define a recurrence for \alg{BUILD-BST}, \(T(n)\),
        and then show that \(T(n) \in O(n^2)\) by showing that \(T(n) \leq c
        \cdot n^2\) for some constant \(c > 0\) and all \(n \geq n_0\).
        \\

        To prove the worst case, we need to understand how \alg{BST.add()}
        works.  It recurses into either the left or right subtree dependent on
        the values of each node.
        \\

        To illicit worst case run-time, it follows that during each recursion,
        we only cut down the size of our problem by 1. This would give the
        recurrence as follows: \(T(n) = T(n - 1) + \Theta(1)\) where the work
        done at each step is constant. By using the substitution method we get
        the following:
        \[
            \begin{split}
                T(n) &= T(n - 1) + \Theta(1)
                \\
                &\leq (c \cdot (n - 1))^2 + \Theta(1)
                \\
                &= c^2 \cdot (n^2 - 2n + 1) + \Theta(1)
                \\
                &\leq c \cdot n^2 - c \cdot 2n + c + \Theta(1)
                \\
                &\leq c \cdot n^2
            \end{split}
        \]

        By using the substitution method we can see that the worst case run
        time of \alg{BUILD-BST} is \(O(n^2)\) and thus our proof is complete.
    \end{proof}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Consider an AVL tree where every node in the tree has a balance factor of
    0. Prove using induction that any such AVL tree is also full: the
    \(d^{th}\) level of the tree (where the root is in level 0) has either
    \(2^d\) or 0 nodes.
    \\

    \textbf{Solution}

    \begin{proof}
        Prove that an AVL tree with a balance factor of 0 for every node is
        full. A full binary tree has either \(2^d\) or 0 nodes at the
        \(d^{th}\) level.
        \\

        \textbf{Base}

        Consider a tree where with a single node, the root, and where it has a
        balance factor of 0. As given, we consider this root node to be at
        depth \(d = 0\).  This gives us \(2^d = 2^0 = 1\) node. Since this is
        perfectly balanced and also consists of just 1 node. Adding any nodes
        would offset the balance factor and would result in being more than 1
        node. Thus the base case holds.
        \\

        \textbf{Step}

        We wish to show for a AVL tree with a depth of \(d\), if we expand it
        so that the depth equals \(d + 1)\) and the balance factor is still 0,
        it must create a ``full'' binary tree in that at the last depth, \(d +
        1\) there will still be \(2^{d + 1}\) nodes.
        \\

        Consider two AVL trees \(T_1\) and \(T_2\) both with a depth \(d\) and
        balance factors of 0. If we were to construct a new AVL tree \(T\) by
        taking a new node and adding \(T_1\) and \(T_2\) as subtrees to the new
        root, we would end up with a new tree. The new tree would then have a
        depth of \(d + 1\) because the new root adds one to the depth. Since
        both \(T_1\) and \(T_2\) both have the same number of nodes, our new
        tree has a balance factor of 0.
        \\

        Since we are taking two AVL trees with balance factors of 0, we get the
        following at the last level:
        \[
            \begin{split}
                \mbox{nodes at level } d + 1 &= \mbox{nodes at level d of } T_1 + \mbox{nodes at level d of } T_2
            \end{split}
        \]

        By assuming the induction hypothesis we get the following:
        \[
            \begin{split}
                \mbox{nodes at level } d + 1 &= 2^d + 2^d
                \\
                &= 2 \cdot 2^d
                \\
                &= 2^{d + 1}
            \end{split}
        \]

        Thus we can see that adding any more nodes would unbalance the tree and
        it wouldn't be full anymore. Thus the proof is complete.
    \end{proof}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Write pseudocode that when given a list of elements sorted in ascending
    order, return a complete BST that contains precisely the elements in the
    input list. The algorithm should run in linear time.
    \\

    \textbf{Solution}

    The algorithm works by taking the median of the list and then setting it as
    the root element in the BST then recurses until the range consists of only
    one element. The pseudocode of the algorithm is below:
    \\

    \begin{linenumbers}[1]
        \renewcommand\linenumberfont{\normalfont\bfseries\small}
        \begin{algorithmic}
            \Function{BuildBST}{$list, start, end$}
                \If{$start > end$}
                    \State{} \Return{NULL}
                \ElsIf{$start = end$}
                    \State{} \Return{} new \Call{Node}{$list[start]$}
                \EndIf{}
                \\
                \State{} $mid \gets start + (end - start) / 2$
                \State{} $root \gets$ new \Call{Node}{$list[mid]$}
                \\
                \State{} $root.left \gets$ \Call{BuildBST}{$list, start, mid - 1$}
                \State{} $root.right \gets$ \Call{BuildBST}{$list, mid + 1, end$}
                \\
                \State{} \Return{$root$}
            \EndFunction{}
        \end{algorithmic}
    \end{linenumbers}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Let \(H\) be an empty hash table with \(n = 6\) bins. Assume that \(H\)
    stores integers according to the hash function \(h(x) = x\) and the probing
    function:
    \[
        P(x, i) = \left\{
            \begin{array}{ll}
                (P(x, i - 1) + i) \bmod n & \quad i > 0
                \\
                h(x) \bmod n & \quad i = 0
            \end{array}
        \right.
    \]

    \begin{enumerate}
        \item Show the configuration of \(H\) after the integers 6, 1, 3, 10,
            and 4 are added.
        \item What happens when the table is checked for the inclusion of the
            integer 0? Explain your answer.
    \end{enumerate}

    \textbf{Solution}
    \\

    \textbf{Part One}

    The locations as follows:

    \begin{enumerate}
        \item When 6 is to be added: the place is \(P(6, 0) = 6 \bmod 6 = 0\)
            which is empty and is placed there. \(H = [6, null, null, null,
            null, null]\)

        \item When 1 is to be added: the place is \(P(1, 0) = 1 \bmod 6 = 1\)
            which is empty and is placed there. \(H = [6, 1, null, null, null,
            null]\)

        \item When 3 is to be added: the place is \(P(3, 0) = 3 \bmod 6 = 3\)
            which is empty and is placed there. \(H = [6, 1, null, 3, null,
            null]\)

        \item When 10 is to be added: the place is \(P(10, 0) = 10 \bmod 6 =
            4\) which is empty and is placed there. \(H = [6, 1, null, 3, 10,
            null]\)

        \item When 4 is to be added: the place is \(P(4, 0) = 4 \bmod 6 = 4\)
            which isn't empty. Therefore we probe again: \(P(4, 1) = (P(4, 0) +
            1) \bmod 6 = (4 + 1) \bmod 6 = 5\) which is empty and it is placed
            there. \(H = [6, 1, null, 3, 10, 4]\).
    \end{enumerate}

    \textbf{Part Two}

    When the table is checked for the inclusion of 0, we need to look calculate
    the hash of where 0 should be placed. We then check to see if 0 is in that
    place. If nothing has been placed in that location of the hash table, we
    know that 0 cannot be anywhere. However, if a value is there that isn't 0,
    we need to run the probing until we run into a value that doesn't exist.
    \\

    The sequence of probes and checking will look like the following:

    \begin{enumerate}
        \item First we check \(P(0, 0) = 0 \bmod 6 = 0\) to see if it is empty.
            It isn't \(null\) and \(0 \neq 6\). Therefore we continue.

        \item Second we check \(P(0, 1) = (P(0, 0) + 1) \bmod 6 = (0 + 1) \bmod
            6 = 1\) to see if it is empty. It isn't \(null\) and \(0 \neq 1\).
            Therefore we continue.

        \item Third we check \(P(0, 2) = (P(0, 1) + 1) \bmod 6 = (1 + 1) \bmod
            6 = 2\) to see if it is empty. It is \(null\) which ends the
            probing sequence. Therefore the check would return \(false\)
            because 0 cannot be in the hash table under open addressing.
    \end{enumerate}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Consider the algorithm given to traverse a rooted binary tree where
    \alg{VISIT} is a constant-time function. Using the master theorem, give a
    tight bound on the running time of \alg{TRAVERSE} as a function of the number
    of nodes in \(t\).
    \\

    \textbf{Solution}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Consider the following recurrence relation:
    \[
        T(n) = \left\{
            \begin{array}{ll}
                1 & \quad n = 1 \\
                7 \cdot T(\frac{n}{2}) + 1 & \quad n > 1
            \end{array}
        \right.
    \]

    \begin{enumerate}
        \item Use the master theorem to show that \(T(n) \in \Theta(n^{\log_2 7})\).
        \item Use induction to show that \(T(n) = \dfrac{1}{6} \cdot (7 \cdot
              n^{\log_2 7} - 1)\). You may assume that \(n\) is a power of two.
    \end{enumerate}

    \textbf{Solution}
    \\

    \textbf{Part One}
    We have \(a = 7\), \(b = 2\) and \(f(n) = 1\). According to the Master
    Method, this gives us:
    \[
        n^{\log_b a} = n^{\log_2 7} \in \Theta(n^{\log_2 7})
    \]

    This puts us in case 1 of the master method because \(f(n) = O(g(n))\)
    where \(f(n) = 1\) and when \(\epsilon = 3\), \(g(n) = n^{\log_2 7 -
    \epsilon} = n^{\log_2 7 - 3} = n^{\log_2 4} = n^2\).
    \\

    To prove that we are in case 1, we just need to find a \(c\) and \(n_0\)
    such that for all \(n > n_0\) the inequality \(1 \leq c \cdot n^2\) holds.
    If we let \(n_0 = 1\) and \(c = 1\) the inequality holds, thus proving
    \(f(n) = O(g(n))\).
    \\

    Now that we've verified that we are in case 1 of the master method, we now know
    that \(T(n) = \Theta(n^{\log_2 7})\).
    \\


    \textbf{Part Two}

    \begin{proof}
        We will use induction to prove that \(T(n) = \dfrac{1}{6} \cdot (7
        \cdot n^{\log_2 7} - 1)\).
        \\

        \textbf{Base}

        Consider the base case when \(n = 1\). It is trivial because \(T(1) = 1\) which is
        equal to \(\dfrac{1}{6}(7n^{log_2 7} - 1) = \dfrac{1}{6}(7 \cdot
        1^{log_2 7} - 1) = \dfrac{1}{6}(7 - 1) = 1\).
        \\

        \textbf{Step}

        To prove the induction step, we can assume that \(n > 1\) and that
        \(n\) is a power of two. The induction hypothesis is thus \(T(n) =
        \dfrac{1}{6}(7{n}^{log_2 7} - 1)\). This give us:
        \[
            \begin{split}
                T(n) &= 7 \cdot T(n/2) + 1
                \\
                &= 7 (\frac{1}{6}(7{(n/2)}^{log_2 7} - 1)) + 1
                \\
                &= 7 (\frac{1}{6}({n}^{log_2 7} - 1)) + 1
                \\
                &= 7 (\frac{1}{6}{n}^{log_2 7} - \frac{1}{6}) + 1
                \\
                &= \frac{7}{6}{n}^{log_2 7} - \frac{7}{6} + 1
                \\
                &= \frac{1}{6}(7{n}^{log_2 7} - 7 + 6)
                \\
                &= \frac{1}{6}(7{n}^{log_2 7} - 1)
            \end{split}
        \]
    \end{proof}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Using the given operations of a priority queue do the following:

    \begin{enumerate}
        \item Provide a pseudocode implementation of the \alg{enqueue\((e)\)}
            and \alg{dequeue\(()\)} operations of a queue that uses a priority
            queue to store its data.
        \item Provide pseudocode implementations of the \alg{push\((e)\)} and
            \alg{pop\(()\)} operations of a stack that uses a priority queue to
            store its data.
    \end{enumerate}

    \textbf{Solution}

    Higher priority means a lower number for this pseudocode.
    \\


    \textbf{Part One}

    \begin{linenumbers}[1]
        \renewcommand\linenumberfont{\normalfont\bfseries\small}
        \begin{algorithmic}
            \State{} $index \gets 0$
            \State{} $queue \gets$ new \Call{PriorityQueue}{ }
            \\
            \Function{enqueue}{$e$}
                \State{} $queue$.\Call{enqueue}{$e, index$}
                \State{} $index \gets index + 1$
            \EndFunction{}
            \\
            \Function{dequeue}{ }
                \State{} \Return{$queue$.\Call{dequeue}{ }}
            \EndFunction{}
        \end{algorithmic}
    \end{linenumbers}

    \textbf{Part Two}

    \begin{linenumbers}[1]
        \renewcommand\linenumberfont{\normalfont\bfseries\small}
        \begin{algorithmic}
            \State{} $index \gets INT\_MAX$
            \State{} $queue \gets$ new \Call{PriorityQueue}{ }
            \\
            \Function{enqueue}{$e$}
                \State{} $queue.$\Call{enqueue}{$e, index$}
                \State{} $index \gets index - 1$
            \EndFunction{}
            \\
            \Function{dequeue}{ }
                \State{} \Return{$queue$.\Call{dequeue}{ }}
            \EndFunction{}
        \end{algorithmic}
    \end{linenumbers}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    Consider the algorithm given for quicksort. Using the knowledge that
    quicksort depends on the behavior of \alg{SELECT-PIVOT}, assume that
    \alg{SELECT-PIVOT} runs in constant time and \alg{PARTITION} runs in time
    \(\Omega(end - start)\), do the following:

    \begin{enumerate}
        \item What \alg{SELECT-PIVOT} behavior will lead to worst case \alg{QUICKSORT} behavior? Use the master
              theorem to derive a tight bound on \alg{QUICKSORT}'s worst case running time.
        \item What \alg{SELECT-PIVOT} behavior will lead to the best case \alg{QUICKSORT} behavior? Use the
              master theorem to derive a tight bound on \alg{QUICKSORT}'s best case running time.
    \end{enumerate}

    \textbf{Solution}
    \\

    \textbf{Part One}

    To derive a tight bound on the worst case running time, we need to come up
    with a recurrence that captures the worst case running time of the
    algorithm.
    \\

    Since \alg{QUICKSORT} is broken up into two subproblems, from \(start\) to
    \(mid - 1\) and then \(mid\) to \(end\), the worst thing that could happen
    is if the mid selected is equal to the end and only cuts down the list by a
    single element each time. This would cause the recurrence to be the
    following:
    \[
        \begin{split}
            T(n) &= T(n - 1) + T(0) + \Theta(end - start)
            \\
            &\leq T(n - 1) + T(0) + \Theta(n)
            \\
            &= T(n - 1) + \Theta(n)
        \end{split}
    \]

    We can't use the master method in this case but we can either use the
    substitution method which will give us an upper bound of \(\Theta(n^2)\).
    Intuitively, this makes sense because for every recursive call from \(n\)
    to 0, we do \(\Theta(n)\) work which gives us a quadratic run time.
    \\

    \textbf{Part Two}

    To derive a tight bound on the best case running time, we will come up with
    a recurrence again.
    \\

    In the best case, the divide and conquer will operate the best when each
    problem is as close to the same size as possible. This means that it will
    half the problem size for each recursive call. This gives us the recurrence
    of the following:
    \[
        \begin{split}
            T(n) &= T(n/2) + T(n/2) + \Theta(end - start)
            \\
            &\leq T(n/2) + T(n/2) + \Theta(n)
            \\
            &= 2 \cdot T(n/2) + \Theta(n)
        \end{split}
    \]

    Using the master method, this gives us \(a = 2\), \(b = 2\) and \(f(n) \in
    \Theta(n)\). Thus \(n^{\log_b a} = n^{\log_2 2} = n^1 = n\). Since \(f(n)
    \in \Theta(n)\), this puts us in the second case. The second case then
    gives us the following best case run time: \(T(n) = \Theta(n^{\log_b a} \lg
    n) = \Theta(n^{\log_2 a} \lg n) = \Theta(n \lg n)\).
    \\

    Since we've found the worst case and best case running times, our solution
    is now complete.
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    A 10-foot rope is stretched across a gap. \(n\) ants are distributed on the
    rope and each ant faces either direction. At the same instant, the ants
    begin to walk, moving forward at a rate of 10 feet per minute. If two ants
    collide, they immediately reverse direction. When an ant reaches the end of
    the rope, it steps off the rope and can no longer block the other ants.

    \begin{enumerate}
        \item What is the largest possible amount of time that can elapse
              before the last ant steps off the rope?
        \item In what situation does the worst-case time arise?
    \end{enumerate}

    \textbf{Solution}
    \\

    \textbf{Part One}

    When two ants bounce off each other, it has no affect on the actual travel
    times. If the ants didn't bounce and just continued to travel, the solution
    would still be the same. Therefore since the rope is 10 feet long and the
    ants travel at 10 ft per minute, the longest time it would take for the
    rope to be clear of ants is 1 minute.
    \\

    \textbf{Part Two}

    The worst case time would arise whenever an ant is placed at the very edge
    of the rope. The ant that will be left on the rope last will always be the
    ant that is farthest from the respective end that it faces.
    \\

    Therefore even if there are two ants placed on the edge of the rope facing
    inwards, they will both fall off at after 1 minute because they will meet
    in the middle and then reverse and travel the remaining half of the rope.
\end{homeworkProblem}

\end{document}
